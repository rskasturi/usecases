{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d20da1-3eab-487b-b226-48047175d196",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0 Copyright (c) 2023, Shivani karangula karangulax.shivani@intel.com, Nikhila Haridas nikhilax.haridas@intel.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df15bf6-8038-4bb3-97ab-9fdc684a5851",
   "metadata": {},
   "source": [
    "\n",
    "# Automatic Speech Recognition Quantization using Intel® Neural Compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b65e3-cf2a-4e91-94f0-ed3db7888161",
   "metadata": {},
   "source": [
    "## Intel® Neural Compressor\n",
    "\n",
    "* Intel® Neural Compressor performs model optimization to reduce the model size and increase the speed of deep learning inferenc for deployment on CPUs or GPUs.\n",
    "  \n",
    "* Intel® Neural Compressor is an open source Python* library that performs model compression techniques such as quantization, pruning, and knowledge distillation across multiple deep learning frameworks including TensorFlow*, PyTorch*, and ONNX* (Open Neural Network Exchange) Runtime.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf8886-5168-4f7d-a2b0-ae589bdb1d10",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "Quantization is a deep learning model optimization technique that is used to improve the speed of inference. It reduces the number of bits required by converting a set of real-valued numbers into a lower bit data representation such as int8 and int4. This helps in reducing the memory requirement, cache miss rate, and computational cost of using neural networks, and also in achieving the goal of higher inference performance.\n",
    "\n",
    "Quantization has three different approaches:\n",
    "\n",
    "* Post training dynamic quantization\n",
    "* Post training static quantization\n",
    "* Quantization aware training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ccea8-6ab7-4934-8557-a6a387be3d7c",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "This notebook describes a detailed step-by-step code walkthrough on How to use Intel® Neural Compressor for Quantizing Whisper Model using Post training dynamic quantization approach on **CPU**.\n",
    "* Load the Model and inference before Quantization.\n",
    "* Quantize the Model.\n",
    "* Inference of the Quanized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab961a-48ac-4a47-92e5-559f89adac8c",
   "metadata": {},
   "source": [
    "### Environment setup \n",
    " \n",
    "* This cell helps to create a python virtual environment ```asr-quant```.\n",
    "* Install necessary packages, along with Intel® Extension for PyTorch.\n",
    "* Create a Jupyter kernel for the notebook environment.\n",
    "* Finally select ```Python (asr-quant)``` kernel to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf22d4-a5b4-4790-867a-599ef9ec3574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating python virtual environment\n",
    "!python3 -m venv asr-quant\n",
    "\n",
    "print(\"Virtual environment created\")\n",
    "\n",
    "print(\"Installing the required dependencies\")\n",
    "\n",
    "# Installing required dependencies in the virtual environment\n",
    "!asr-quant/bin/pip install transformers==4.44.2 \\\n",
    "                            pandas==2.2.2 \\\n",
    "                            numpy==1.26.4 \\\n",
    "                           datasets==3.0.0 \\\n",
    "                           evaluate==0.4.2 \\\n",
    "                           jiwer==3.0.4 \\\n",
    "                           librosa==0.10.2.post1 \\\n",
    "                           neural_compressor==3.0.2 \\\n",
    "                           intel-extension-for-pytorch \\\n",
    "                           torch==2.3.1+cxx11.abi \\\n",
    "                           intel-extension-for-pytorch==2.3.110+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n",
    "\n",
    "print(\"Dependencies installed\")\n",
    "\n",
    "# Install `ipykernel` to register the virtual environment as a Jupyter kernel\n",
    "!asr-quant/bin/pip install ipykernel\n",
    "\n",
    "# Register the virtual environment as a Jupyter kernel\n",
    "!asr-quant/bin/python -m ipykernel install --user --name=asr-quant --display-name \"Python (asr-quant)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0630ff-4473-4e02-8926-a7f626058950",
   "metadata": {},
   "source": [
    "## Let's patch few changes to source\n",
    "\n",
    "* Few changes need to be done in modeling_whisper.py & generation_whisper.py inorder to run the quantization script.Run the below two scripts to make the necessary changes.\n",
    "* Line numbers may vary based on the transformer version used.Transformer version 4.44.2 used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da4198-a718-4444-9557-a6bf19701d1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def patch_file(file_path, line_number, original_line, replacement_text):\n",
    "    \"\"\"\n",
    "    Replaces a specific line in a file with a given replacement text if the original line matches.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the file to be modified.\n",
    "        line_number (int): Line number where the replacement should happen (1-based indexing).\n",
    "        original_line (str): The exact line content to be replaced.\n",
    "        replacement_text (str): The new multi-line text to replace the original line.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        line_index = line_number - 1\n",
    "        \n",
    "        if len(lines) > line_index and lines[line_index].strip() == original_line.strip():\n",
    "            \n",
    "            lines[line_index] = replacement_text\n",
    "            \n",
    "            with open(file_path, 'w') as file:\n",
    "                file.writelines(lines)\n",
    "            print(f\"Replacement on line {line_number} completed in {file_path}!\")\n",
    "        else:\n",
    "            print(f\"Original line not found at line {line_number} in {file_path}. No changes made.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while modifying the file {file_path}: {e}\")\n",
    "\n",
    "# first file modeling_whisper.py\n",
    "\n",
    "patch_file(\n",
    "    file_path='./asr-quant/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py',\n",
    "    line_number=1069,\n",
    "    original_line=\"expected_seq_length = self.config.max_source_positions * self.conv1.stride[0] * self.conv2.stride[0]\\n\",\n",
    "    replacement_text=\"\"\"        try:\n",
    "            expected_seq_length = self.config.max_source_positions * self.conv1.stride[0] * self.conv2.stride[0]\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                expected_seq_length = self.config.max_source_positions * self.conv1.module.module.stride[0] * self.conv2.module.module.stride[0]\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    expected_seq_length = self.config.max_source_positions * self.conv1.module.module.module.stride[0] * self.conv2.module.module.module.stride[0]\n",
    "                except AttributeError:\n",
    "                    expected_seq_length = None\n",
    "# REPLACEMENT_DONE\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# second file generation_whisper.py\n",
    "patch_file(\n",
    "    file_path='./asr-quant/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py',\n",
    "    line_number=505,\n",
    "    original_line=\"input_stride = self.model.encoder.conv1.stride[0] * self.model.encoder.conv2.stride[0]\\n\",\n",
    "    replacement_text=\"\"\"        try:\n",
    "            input_stride = self.model.encoder.conv1.stride[0] * self.model.encoder.conv2.stride[0]\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                input_stride = self.model.encoder.conv1.module.module.stride[0] * self.model.encoder.conv2.module.module.stride[0]\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    input_stride = self.model.encoder.conv1.module.module.module.stride[0] * self.model.encoder.conv2.module.module.module.stride[0]\n",
    "                except AttributeError:\n",
    "                    input_stride = None\n",
    "#REPLACEMENT_DONE\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18583e46-638c-441d-ad00-36b5650b957b",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b511b70-43bb-43ab-9865-3a81a49f4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration,pipeline\n",
    "from datasets import load_dataset\n",
    "from neural_compressor import quantization\n",
    "from neural_compressor.quantization import fit\n",
    "from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion\n",
    "from neural_compressor.utils.pytorch import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d9e7f-826e-43d7-8e59-f610120ba219",
   "metadata": {},
   "source": [
    "### Whisper Model Inference before Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521013c4-d3d4-4788-a088-dabbd4995b1b",
   "metadata": {},
   "source": [
    "**Model Card** : https://huggingface.co/openai/whisper-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7d3b53-1ca3-40d1-a0e5-fbd9b2f2d1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n",
      "Time spent before quantization :116.60087156295776 seconds\n"
     ]
    }
   ],
   "source": [
    "model_id='openai/whisper-large'\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id,use_safetensors=True)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-large')\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "start_time = time.time()\n",
    "\n",
    "predicted_ids = model.generate(input_features)\n",
    "\n",
    "end_time=time.time()\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)\n",
    "print(f\"Time spent before quantization :{end_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19da00a3-6914-4b83-8cd8-1d69cd2cd631",
   "metadata": {},
   "source": [
    "### Quantization of Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de295bd-eaf0-421d-b884-f7075e4e3883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:21:30 [INFO] Start auto tuning.\n",
      "2024-12-10 09:21:30 [INFO] Quantize model without tuning!\n",
      "2024-12-10 09:21:30 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2024-12-10 09:21:30 [INFO] Adaptor has 5 recipes.\n",
      "2024-12-10 09:21:30 [INFO] 0 recipes specified by user.\n",
      "2024-12-10 09:21:30 [INFO] 3 recipes require future tuning.\n",
      "2024-12-10 09:21:30 [INFO] *** Initialize auto tuning\n",
      "2024-12-10 09:21:30 [INFO] {\n",
      "2024-12-10 09:21:30 [INFO]     'PostTrainingQuantConfig': {\n",
      "2024-12-10 09:21:30 [INFO]         'AccuracyCriterion': {\n",
      "2024-12-10 09:21:30 [INFO]             'criterion': 'relative',\n",
      "2024-12-10 09:21:30 [INFO]             'higher_is_better': True,\n",
      "2024-12-10 09:21:30 [INFO]             'tolerable_loss': 0.1,\n",
      "2024-12-10 09:21:30 [INFO]             'absolute': None,\n",
      "2024-12-10 09:21:30 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x149f89893f40>>,\n",
      "2024-12-10 09:21:30 [INFO]             'relative': 0.1\n",
      "2024-12-10 09:21:30 [INFO]         },\n",
      "2024-12-10 09:21:30 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2024-12-10 09:21:30 [INFO]         'backend': 'default',\n",
      "2024-12-10 09:21:30 [INFO]         'calibration_sampling_size': [\n",
      "2024-12-10 09:21:30 [INFO]             100\n",
      "2024-12-10 09:21:30 [INFO]         ],\n",
      "2024-12-10 09:21:30 [INFO]         'device': 'cpu',\n",
      "2024-12-10 09:21:30 [INFO]         'domain': 'auto',\n",
      "2024-12-10 09:21:30 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2024-12-10 09:21:30 [INFO]         'excluded_precisions': [\n",
      "2024-12-10 09:21:30 [INFO]         ],\n",
      "2024-12-10 09:21:30 [INFO]         'framework': 'pytorch_fx',\n",
      "2024-12-10 09:21:30 [INFO]         'inputs': [\n",
      "2024-12-10 09:21:30 [INFO]         ],\n",
      "2024-12-10 09:21:30 [INFO]         'model_name': '',\n",
      "2024-12-10 09:21:30 [INFO]         'op_name_dict': None,\n",
      "2024-12-10 09:21:30 [INFO]         'op_type_dict': {\n",
      "2024-12-10 09:21:30 [INFO]             'Embedding': {\n",
      "2024-12-10 09:21:30 [INFO]                 'weight': {\n",
      "2024-12-10 09:21:30 [INFO]                     'dtype': [\n",
      "2024-12-10 09:21:30 [INFO]                         'fp32'\n",
      "2024-12-10 09:21:30 [INFO]                     ]\n",
      "2024-12-10 09:21:30 [INFO]                 },\n",
      "2024-12-10 09:21:30 [INFO]                 'activation': {\n",
      "2024-12-10 09:21:30 [INFO]                     'dtype': [\n",
      "2024-12-10 09:21:30 [INFO]                         'fp32'\n",
      "2024-12-10 09:21:30 [INFO]                     ]\n",
      "2024-12-10 09:21:30 [INFO]                 }\n",
      "2024-12-10 09:21:30 [INFO]             }\n",
      "2024-12-10 09:21:30 [INFO]         },\n",
      "2024-12-10 09:21:30 [INFO]         'outputs': [\n",
      "2024-12-10 09:21:30 [INFO]         ],\n",
      "2024-12-10 09:21:30 [INFO]         'quant_format': 'default',\n",
      "2024-12-10 09:21:30 [INFO]         'quant_level': 'auto',\n",
      "2024-12-10 09:21:30 [INFO]         'recipes': {\n",
      "2024-12-10 09:21:30 [INFO]             'smooth_quant': False,\n",
      "2024-12-10 09:21:30 [INFO]             'smooth_quant_args': {\n",
      "2024-12-10 09:21:30 [INFO]             },\n",
      "2024-12-10 09:21:30 [INFO]             'layer_wise_quant': False,\n",
      "2024-12-10 09:21:30 [INFO]             'layer_wise_quant_args': {\n",
      "2024-12-10 09:21:30 [INFO]             },\n",
      "2024-12-10 09:21:30 [INFO]             'fast_bias_correction': False,\n",
      "2024-12-10 09:21:30 [INFO]             'weight_correction': False,\n",
      "2024-12-10 09:21:30 [INFO]             'gemm_to_matmul': True,\n",
      "2024-12-10 09:21:30 [INFO]             'graph_optimization_level': None,\n",
      "2024-12-10 09:21:30 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2024-12-10 09:21:30 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2024-12-10 09:21:30 [INFO]             'pre_post_process_quantization': True,\n",
      "2024-12-10 09:21:30 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2024-12-10 09:21:30 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2024-12-10 09:21:30 [INFO]             ],\n",
      "2024-12-10 09:21:30 [INFO]             'dedicated_qdq_pair': False,\n",
      "2024-12-10 09:21:30 [INFO]             'rtn_args': {\n",
      "2024-12-10 09:21:30 [INFO]             },\n",
      "2024-12-10 09:21:30 [INFO]             'awq_args': {\n",
      "2024-12-10 09:21:30 [INFO]             },\n",
      "2024-12-10 09:21:30 [INFO]             'gptq_args': {\n",
      "2024-12-10 09:21:30 [INFO]             },\n",
      "2024-12-10 09:21:30 [INFO]             'teq_args': {\n",
      "2024-12-10 09:21:30 [INFO]             },\n",
      "2024-12-10 09:21:30 [INFO]             'autoround_args': {\n",
      "2024-12-10 09:21:30 [INFO]             }\n",
      "2024-12-10 09:21:30 [INFO]         },\n",
      "2024-12-10 09:21:30 [INFO]         'reduce_range': None,\n",
      "2024-12-10 09:21:30 [INFO]         'TuningCriterion': {\n",
      "2024-12-10 09:21:30 [INFO]             'max_trials': 5,\n",
      "2024-12-10 09:21:30 [INFO]             'objective': [\n",
      "2024-12-10 09:21:30 [INFO]                 'performance'\n",
      "2024-12-10 09:21:30 [INFO]             ],\n",
      "2024-12-10 09:21:30 [INFO]             'strategy': 'basic',\n",
      "2024-12-10 09:21:30 [INFO]             'strategy_kwargs': None,\n",
      "2024-12-10 09:21:30 [INFO]             'timeout': 0\n",
      "2024-12-10 09:21:30 [INFO]         },\n",
      "2024-12-10 09:21:30 [INFO]         'use_bf16': True,\n",
      "2024-12-10 09:21:30 [INFO]         'ni_workload_name': 'quantization'\n",
      "2024-12-10 09:21:30 [INFO]     }\n",
      "2024-12-10 09:21:30 [INFO] }\n",
      "2024-12-10 09:21:30 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2024-12-10 09:21:38 [INFO]  Found 32 blocks\n",
      "2024-12-10 09:21:38 [INFO] Attention Blocks: 32\n",
      "2024-12-10 09:21:38 [INFO] FFN Blocks: 32\n",
      "2024-12-10 09:21:38 [INFO] Pass query framework capability elapsed time: 7919.58 ms\n",
      "2024-12-10 09:21:38 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2024-12-10 09:21:38 [INFO] Quantize the model with default config.\n",
      "2024-12-10 09:21:38 [INFO] Convert operators to bfloat16\n",
      "2024-12-10 09:21:38 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n",
      "2024-12-10 09:22:17 [INFO] |*******Mixed Precision Statistics*******|\n",
      "2024-12-10 09:22:17 [INFO] +-----------+-------+------+------+------+\n",
      "2024-12-10 09:22:17 [INFO] |  Op Type  | Total | INT8 | BF16 | FP32 |\n",
      "2024-12-10 09:22:17 [INFO] +-----------+-------+------+------+------+\n",
      "2024-12-10 09:22:17 [INFO] |   Linear  |  513  | 513  |  0   |  0   |\n",
      "2024-12-10 09:22:17 [INFO] |   Conv1d  |   2   |  0   |  2   |  0   |\n",
      "2024-12-10 09:22:17 [INFO] | Embedding |   2   |  0   |  0   |  2   |\n",
      "2024-12-10 09:22:17 [INFO] +-----------+-------+------+------+------+\n",
      "2024-12-10 09:22:17 [INFO] Pass quantize model elapsed time: 39569.5 ms\n",
      "2024-12-10 09:22:17 [INFO] Save tuning history to /home/u8bd311b633876ba392b704069aeab3e/Neural_compressor/nc_workspace/2024-12-10_09-18-59/./history.snapshot.\n",
      "2024-12-10 09:22:17 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2024-12-10 09:22:17 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2024-12-10 09:22:17 [INFO] Save deploy yaml to /home/u8bd311b633876ba392b704069aeab3e/Neural_compressor/nc_workspace/2024-12-10_09-18-59/deploy.yaml\n",
      "2024-12-10 09:22:24 [INFO] Save config file and weights of quantized model to /home/u8bd311b633876ba392b704069aeab3e/Neural_compressor/quantized_whisper_large.\n"
     ]
    }
   ],
   "source": [
    "model_id='openai/whisper-large'\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_id,use_safetensors=True)\n",
    "output_dir = \"quantized_whisper_large\"\n",
    "\n",
    "tune=True\n",
    "\n",
    "if tune:\n",
    "    tuning_criterion = TuningCriterion(max_trials=5)\n",
    "    accuracy_criterion = AccuracyCriterion(tolerable_loss=0.1)\n",
    "    op_type_dict = {\n",
    "        \"Embedding\": {\"weight\": {\"dtype\": [\"fp32\"]}, \"activation\": {\"dtype\": [\"fp32\"]}}\n",
    "        }\n",
    "    conf = PostTrainingQuantConfig(approach=\"dynamic\", tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion,op_type_dict=op_type_dict)\n",
    "    q_model = quantization.fit(model, conf=conf) \n",
    "    q_model.save(output_dir)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51871c9-dc5a-40ff-ba10-34b0c4338e33",
   "metadata": {},
   "source": [
    "### Whisper Model Inference after Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dae326c-bedf-458f-abf6-1b3b50d0859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u8bd311b633876ba392b704069aeab3e/Neural_compressor/asr-quant/lib/python3.10/site-packages/torch/_utils.py:403: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n",
      "2024-12-10 09:23:06 [INFO] Convert operators to bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n",
      "Time spent after quantization :104.29494643211365 seconds\n"
     ]
    }
   ],
   "source": [
    "model_id='openai/whisper-large'\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id,use_safetensors=True)\n",
    "\n",
    "# Load the quantized model\n",
    "model = load(os.path.abspath(os.path.expanduser('./quantized_whisper_large')), model)\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-large')\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "\n",
    "end_time=time.time()\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)\n",
    "print(f\"Time spent after quantization :{end_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa16b40-ce90-49f8-af4c-9813da7bcf3e",
   "metadata": {},
   "source": [
    "### Benchmark results of verified whisper models\n",
    "\n",
    "| **Model**                 | **Original Size** | **Quantized Size** |\n",
    "|---------------------------|-------------------|--------------------|\n",
    "| openai--whisper-large     | 5.8G              | 1.7G               |\n",
    "| openai--whisper-small     | 927M              | 393.8M             |\n",
    "| openai--whisper-large-v3  | 2.9G              | 1.7G               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87985896-1024-4472-ac29-dfe52f915cdf",
   "metadata": {},
   "source": [
    "### Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like Camel-5B and OpenLLaMA 3b v2 are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "For detailed information on each model's capabilities, licensing, and attribution, please refer to the respective model cards:\n",
    "\n",
    "* **openai/whisper-large**\n",
    "  \n",
    "   * Model card : https://huggingface.co/openai/whisper-large\n",
    "\n",
    "* **openai/whisper-small**\n",
    "\n",
    "   * Model card : https://huggingface.co/openai/whisper-small\n",
    "\n",
    "* **openai/whisper-large-v3**\n",
    "\n",
    "   * Model card : https://huggingface.co/openai/whisper-large-v3\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asr-quant)",
   "language": "python",
   "name": "asr-quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
