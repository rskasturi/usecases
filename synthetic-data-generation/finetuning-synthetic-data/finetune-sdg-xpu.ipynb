{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacb957e-5326-4eba-9bed-e5f8cdabd7e9",
   "metadata": {},
   "source": [
    "# Small Language as a Comedy Dialogue Generator\n",
    "* Are you curious about how you can use Small Language models <1B to act as Funny Dialogue Generator based on the persona you provide.\n",
    "* Here we used [vLLM](https://github.com/vllm-project/vllm) to generate Synthetic Data with an LLM, you can check how to do that [here](https://github.com/rskasturi/usecases/tree/master/synthetic-data-generation).\n",
    "* Powered by Intel® Data Center GPU Max 1100s, this notebook provides a hands-on experience that doesn't require deep technical knowledge. Whether you're a student, writer, educator, or just curious about AI, this guide is designed for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeae9c4-a0f1-4d6e-8710-e2119e5b23b5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, you will learn how to fine-tune a Small language model (Qwen) using Intel Max Series GPUs (XPUs) for a specific task. The notebook covers the following key points:\n",
    "\n",
    "1. Setting up the environment and optimizing it for Intel GPUs\n",
    "2. Initializing the XPU and configuring LoRA settings for efficient fine-tuning\n",
    "3. Loading the pre-trained Qwen model and testing its performance\n",
    "4. Preparing a diverse dataset of question-answer pairs covering a specific domain\n",
    "5. Fine-tuning the model using the Hugging Face Trainer class\n",
    "6. Evaluating the fine-tuned model on a test dataset\n",
    "7. Saving and loading the fine-tuned model for future use\n",
    "8. The notebook demonstrates how fine-tuning can enhance a model's performance on a diverse range of topics, making it more versatile and applicable to various domains. You will gain insights into the process of creating a task-specific model that can provide accurate and relevant responses to a wide range of questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5fed1-0e74-467a-8a94-c0c10dc8979a",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up the Environment 🛠️\n",
    "First things first, let's get our environment ready! We'll import all the necessary packages, including the Hugging Face transformers library, datasets for easy data loading, wandb for experiment tracking, and a few others. \n",
    "\n",
    "We'll now make sure to optimize our environment for the Intel GPU by setting the appropriate environment variables and configuring the number of cores and threads. This will ensure we get the best performance out of our hardware! ⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7b5f1-a9bb-4e9e-98b8-fc6921ba9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14087416-0da9-420a-88f2-abaf6628dbfe",
   "metadata": {},
   "source": [
    "### Step 2: Initializing the XPU 🎮\n",
    "Next, we'll initialize the Intel Max 1110 GPU, which is referred to as an XPU. We'll use the intel_extension_for_pytorch library to seamlessly integrate XPU namespace with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f0166-790f-44e2-8892-c6a82990c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5eecb-f0d0-4bac-a97f-444197f471fd",
   "metadata": {},
   "source": [
    "### Step 3: Configuring the LoRA Settings 🎛️\n",
    "To finetune our Qwen model efficiently, we'll use the LoRA (Low-Rank Adaptation) technique.\n",
    "\n",
    "LoRA allows us to adapt the model to our specific task by training only a small set of additional parameters. This greatly reduces the training time and memory requirements! ⏰\n",
    "\n",
    "We'll define the LoRA configuration, specifying the rank (r) and the target modules we want to adapt. 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11346fb-b56a-45b2-adba-a6f4c87cb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # could use q, v and 0 projections as well and comment out the rest\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \n",
    "                    \"v_proj\", \"k_proj\", \n",
    "                    \"gate_proj\", \"up_proj\",\n",
    "                    \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141c1d9-34d3-45d8-9aa0-f91f27821138",
   "metadata": {},
   "source": [
    "### Step 4: Loading the Qwen Model 🤖\n",
    "Now, let's load the Qwen model using the Hugging Face AutoModelForCausalLM class. We'll also load the corresponding tokenizer to preprocess our input data. The model will be moved to the XPU for efficient training. 💪\n",
    "\n",
    "[Model Card](https://huggingface.co/Qwen/Qwen2.5-0.5B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c4053-e0cc-4534-a8a6-f8c381c9dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() else \"cpu\"\n",
    "if USE_CPU:\n",
    "    device = \"cpu\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "# Configure the model's pre-training tensor parallelism degree to match the fine-tuning setup\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde4f854-13b4-4037-9279-e38352930243",
   "metadata": {},
   "source": [
    "### Step 5: Testing the Model 🧪\n",
    "Before we start finetuning, let's test the Qwen model on a sample input to see how it performs out-of-the-box. We'll generate some responses bsaed on a few questions in the test_inputs list below. 🌿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb20d04-b121-4e1f-af7e-2f3abba7e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1250, \n",
    "                         do_sample=False, top_k=100,temperature=0.1, \n",
    "                         eos_token_id=tokenizer.eos_token_id)   \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def test_model(model, test_inputs):\n",
    "    \"\"\"quickly test the model using queries.\"\"\"\n",
    "    for input_text in test_inputs:\n",
    "        print(\"__\"*25)\n",
    "        generated_response = generate_response(model, input_text)\n",
    "        print(f\"{input_text}\")\n",
    "        print(f\"Generated Answer: {generated_response}\\n\")\n",
    "        print(\"__\"*25)\n",
    "\n",
    "test_inputs = [\n",
    "    \"Assume you are an english teacher, can you frame standup comedies using your knowledge, skills, experience, or insights?\",\n",
    "    \"Assume you are a new media reporter from CNN, can you frame standup comedies using your knowledge, skills, experience, or insights?\"\n",
    "    \"Assume you are a software engineer well-versed in C/C++ but new to Fortran, can you frame standup comedies using your knowledge, skills, experience, or insights?\"\n",
    "    \"Assume you are An investigative reporter who wants to uncover the truth behind the spy's past, can you frame standup comedies using your knowledge, skills, experience, or insights?\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model before fine-tuning:\")\n",
    "test_model(model, test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdd90d-a046-4cc4-a37f-5e5e55407049",
   "metadata": {},
   "source": [
    "### Step 6: Preparing the Dataset 📊\n",
    "For finetuning our model, we'll be using a synthetic dataset. This dataset contains a diverse range of persona's. By focusing on the persona(input text) there is comedy dialogue asociated with that perosona.\n",
    "\n",
    "We'll then split the extracted data into training and validation sets using the train_test_split function from the sklearn.model_selection module. This will help us assess the model's performance during the finetuning process. 📊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2c68d-7eda-47b5-9f5b-c1e740cf46e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"comedy_synthesis_15000.jsonl\")[\"train\"]\n",
    "print(dataset[0])\n",
    "print(f\"Persona: {dataset[0]['input persona']}\")\n",
    "print(f\"Synthetic data Generated: {dataset[0]['synthesized text']}\")\n",
    "\n",
    "# Function to format prompts\n",
    "def format_prompts(batch):\n",
    "    formatted_prompts = []\n",
    "    for instruction, response in zip(batch[\"input persona\"], batch[\"synthesized text\"]):\n",
    "        # Correct variable usage in the prompt\n",
    "        prompt = (f\"Instruction:\\n{instruction}\\n\"\n",
    "                  \"Assume you are the persona described above and I want you to act as a stand-up comedian. \"\n",
    "                  \"Write content that reflects your unique voice, expertise, and humor, tailored to your specific field.\\n\"\n",
    "                  f\"Response:\\n{response}\")\n",
    "        formatted_prompts.append(prompt)\n",
    "    return {\"text\": formatted_prompts}\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=99)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69b265-3595-4ded-bfe7-fdcef2026ee1",
   "metadata": {},
   "source": [
    "### Step 7: Finetuning the Model 🏋️‍♂️\n",
    "It's time to finetune our Qwen model! We'll use the SFTTrainer class from the trl library, which is designed for supervised fine-tuning of language models. We'll specify the training arguments, such as batch size, learning rate, and evaluation strategy. 📈\n",
    "\n",
    "Supervised fine-tuning (SFT) is a powerful technique for adapting pre-trained language models to specific tasks. By providing the model with Persona's driven comedy dialogue of 15k dataset, we can guide it to generate more accurate and relevant responses. SFT allows the model to learn the patterns and relationships specific to the diverse range of topics covered in the dataset. 🎓\n",
    "\n",
    "By focusing on the Generated dialogue based on the persona, we can leverage the rich information available in the Comedy Synthesis dataset to enhance our model's ability to provide informative and contextually appropriate responses. The model will learn to understand the nuances and intricacies of various question types and generate answers that are coherent and relevant. 💡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec626a3d-a8ee-4fa7-85b0-b301fffd111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import warnings\n",
    "from transformers import logging as transformers_logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    " \n",
    "from trl import SFTTrainer\n",
    " \n",
    "os.environ[\"IPEX_TILE_AS_DEVICE\"] = \"1\"\n",
    " \n",
    "finetuned_model_id = \"qwen-0.5B-comedy\"\n",
    " \n",
    "# Calculate max_steps based on the subset size\n",
    "num_train_samples = len(train_dataset)\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "steps_per_epoch = num_train_samples // (batch_size * gradient_accumulation_steps)\n",
    "num_epochs = 2\n",
    "max_steps = steps_per_epoch * num_epochs\n",
    "print(f\"Finetuning for max number of steps: {max_steps}\")\n",
    " \n",
    "def print_training_summary(results):\n",
    "    print(f\"Time: {results.metrics['train_runtime']: .2f}\")\n",
    "    print(f\"Samples/second: {results.metrics['train_samples_per_second']: .2f}\")\n",
    "    get_memory_usage()\n",
    " \n",
    "training_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=0.05,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=1e-5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        bf16=True,\n",
    "        logging_steps=100,\n",
    "        output_dir=finetuned_model_id,\n",
    "        use_ipex=True,\n",
    "        max_grad_norm=0.6,\n",
    "        weight_decay=0.01,\n",
    "        group_by_length=True\n",
    ")\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=True\n",
    ")\n",
    " \n",
    "if device != \"cpu\":\n",
    "    torch.xpu.empty_cache()\n",
    "results = trainer.train()\n",
    "print_training_summary(results)\n",
    " \n",
    "# save lora model\n",
    "tuned_lora_model = \"qwen-0.5B-comedy-lora\"\n",
    "trainer.model.save_pretrained(tuned_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510c90c-0e88-4bee-8212-6b8fc0621c5f",
   "metadata": {},
   "source": [
    "### Step 8: Savethe Finetuned Model 💾\n",
    "After finetuning, let's put our model to the test! But before we do that, we need to merge the LoRA weights with the base model. This step is crucial because the LoRA weights contain the learned adaptations from the finetuning process. By merging the LoRA weights, we effectively incorporate the knowledge gained during finetuning into the base model. 🧠💡\n",
    "\n",
    "To merge the LoRA weights, we'll use the merge_and_unload() function provided by the PEFT library. This function seamlessly combines the LoRA weights with the corresponding weights of the base model, creating a single unified model that includes the finetuned knowledge. 🎛️🔧\n",
    "\n",
    "Once the LoRA weights are merged, we'll save the finetuned model to preserve its state. This way, we can easily load and use the finetuned model for future tasks without having to repeat the finetuning process. ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b8666-a76c-4c30-9598-6c7a0ee20272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "tuned_model = \"qwen-0.5B-comedy\"\n",
    "tuned_lora_model = \"qwen-0.5B-comedy-lora\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, tuned_lora_model)\n",
    "model = model.merge_and_unload()\n",
    "# save final tuned model\n",
    "model.save_pretrained(tuned_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "#model2 = ipex.optimize_transformers(model)  # optimize the model using `ipex`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da6e6b-9da2-4a04-bfc2-b9d0ad41499c",
   "metadata": {},
   "source": [
    "### Step 8: Evaluating the Finetuned Model 🎉\n",
    "Now, let's generate a response to the same question we asked earlier using the finetuned model. We'll compare the output with the pre-finetuned model to see how much it has improved. Get ready to be amazed by the power of finetuning! 🤩💫\n",
    "\n",
    "By merging the LoRA weights and saving the finetuned model, we ensure that our model is ready to tackle tasks with its newly acquired knowledge. So, let's put it to the test and see how it performs! 🚀🌟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02488d23-17af-43d4-bdc6-bebcc7d3d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "   \"Assume you are an english teacher, can you frame standup comedies using your knowledge, skills, experience, or insights?\",\n",
    "    \"Assume you are a new media reporter from CNN, can you frame standup comedies using your knowledge, skills, experience, or insights?\",\n",
    "    \"Assume you are a software engineer well-versed in C/C++ but new to Fortran, can you frame standup comedies using your knowledge, skills, experience, or insights?\",\n",
    "    \"Assume you are An investigative reporter who wants to uncover the truth behind the spy's past, can you frame standup comedies using your knowledge, skills, experience, or insights?\"]\n",
    "device = \"xpu:0\"\n",
    "\n",
    "model = model.to(device)\n",
    "for text in test_inputs:\n",
    "    print(\"__\"*25)\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1250, \n",
    "                             do_sample=False, top_k=100,temperature=0.1, \n",
    "                             eos_token_id=tokenizer.eos_token_id)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    print(\"__\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b95242-38da-4acb-8a8b-d85185882e19",
   "metadata": {},
   "source": [
    "### Disclaimer for Using Large Language Models\n",
    "Please be aware that while Large Language Models like Camel-5B and OpenLLaMA 3b v2 are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpu",
   "language": "python",
   "name": "xpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
