{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e680ef-9d7c-4dcb-a8f7-c87ca7468b89",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rajashekar Kasturi <rajashekarx.kasturi@intel.com>, Thasneem Vazim <thasneemx.vazim@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c568a-6efb-4dea-b5f9-0279a1771374",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data using vLLM on IntelÂ® Max Series GPUs ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c085a-3721-43fe-9664-023b7bea54d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ“’ Overview\n",
    "\n",
    "The notebook helps you create synthetic data using vLLM for **comedy dialogue generation** ðŸ˜….\n",
    "1. Setting up the environment of vLLM on IntelÂ® GPUs\n",
    "2. Run sample inference using vLLM\n",
    "3. Approach: Synthetic data generation\n",
    "4. Choose a template to generate synthetic data\n",
    "5. Generate synthetic data for comedy dialogue generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ad66a-9e55-498f-a1b7-33337aec1c83",
   "metadata": {},
   "source": [
    "## Step 1: Setting up environment ðŸ› ï¸\n",
    "\n",
    "First things first, let's get our environment ready! We'll install all the necessary packages, including the IntelÂ® Extension for PyTorch, datasets for easy data loading.ðŸ“¦\n",
    "\n",
    "* Clone the vLLM repository.\n",
    "* Setting up required packages along with IntelÂ® Extension for PyTorch to run on IntelÂ® GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc065d-00e8-4450-8e4a-ae5475ab9e2f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['VLLM_TARGET_DEVICE']='xpu'\n",
    "\n",
    "ROOT_DIR = Path.cwd()\n",
    "\n",
    "def print_cwd():\n",
    "    \"\"\"Prints the current working directory.\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    print(f\"Current directory: {cwd}\")\n",
    "\n",
    "def clone_vllm_repo():\n",
    "    \"\"\"Clones the vllm.git repository with specific options.\"\"\"\n",
    "    print_cwd()\n",
    "    os.system(\"git config --global advice.detachedHead false\")\n",
    "    os.system(\"git clone -b v0.6.2 --depth=1 https://github.com/vllm-project/vllm.git\")\n",
    "    os.system(\"git config --global advice.detachedHead true\")\n",
    "\n",
    "if not os.path.exists(f\"{ROOT_DIR}/vllm/\"):\n",
    "    try:\n",
    "        clone_vllm_repo()\n",
    "        print(\"vllm.git repository cloned successfully!âœ…\")\n",
    "        print(\"Setting up vLLM Environment for Intel GPUs.....âŒ›\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during setup: {e}\")\n",
    "        exit(1)\n",
    "try:\n",
    "    print(\"Changing to vLLM directory.....\")\n",
    "    # Change the current working directory to specified path\n",
    "    os.chdir(f\"{ROOT_DIR}/vllm\")\n",
    "    print_cwd()\n",
    "except OSError as e:\n",
    "    print(f\"Error changing directory: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# Installation commands using subprocess for better error handling and flexibility\n",
    "print(\"vLLM Setup Started!\")\n",
    "os.system(f\"{sys.executable} -m pip cache purge > /dev/null 2>&1\")\n",
    "os.system(f\"{sys.executable} -m pip install --upgrade pip > /dev/null 2>&1\")\n",
    "os.system(f\"\"\"{sys.executable} -m pip install torch==2.3.1+cxx11.abi torchvision==0.18.1+cxx11.abi torchaudio==2.3.1+cxx11.abi intel-extension-for-pytorch==2.3.110+xpu oneccl_bind_pt==2.3.100+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/ > /dev/null 2>&1\"\"\")\n",
    "os.system(f\"{sys.executable} use_existing_torch.py > /dev/null 2>&1\")\n",
    "print(\"Almost there.....!âŒ›âŒ›\")\n",
    "os.system(f\"{sys.executable} -m pip install -v -r requirements-xpu.txt > /dev/null 2>&1\")\n",
    "os.system(f\"{sys.executable} -m pip install setuptools_scm> /dev/null 2>&1\")\n",
    "os.system(f\"{sys.executable} setup.py install > /dev/null 2>&1\")\n",
    "\n",
    "print(\"\\nvLLM environment setup is now ready!! âœ…\")\n",
    "\n",
    "# Change back to the root directory\n",
    "os.chdir(f\"{ROOT_DIR}\")\n",
    "print(\"Changing back to Notebook directory...\\n\")\n",
    "# print_cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f025dc-df8e-48ad-9755-d11df6d23669",
   "metadata": {},
   "source": [
    "## Step 2: Run Sample Inference using vLLM â–¶ï¸\n",
    "\n",
    "1. Here is an example of [offline_inference](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference.py) to run on IntelÂ® GPUs.\n",
    "2. Select the desired model and ```SamplingParameters``` to control llm generated output.\n",
    "3. ```free_memory()``` used to free the allocated resources on GPU.\n",
    "\n",
    "**Note**: Kindly restart the kernel to have changes reflected, if you encounter ```ImportError: cannot import name 'LLM' from 'vllm' (unknown location)``` (Kernel->Restart Kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bfa3a-f04e-4472-b10b-0b6c76f9f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex #to include XPU namespace\n",
    "from vllm import LLM, SamplingParams\n",
    "import gc\n",
    "\n",
    "\n",
    "# Clear cache of the XPU\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "\n",
    "def free_memory(llm_model):\n",
    "    \"\"\"This function free up the gpu memory\n",
    "    {input}: pass the llm object\n",
    "    \"\"\"\n",
    "    # Delete the llm object and free the memory\n",
    "    llm = llm_model\n",
    "    del llm.llm_engine.model_executor\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.xpu.empty_cache()\n",
    "    # print(\"Successfully deleted the llm pipeline and free the GPU memory.\")\n",
    "\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"What are we having for dinner?\",\n",
    "]\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=256)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    print(\"__\" * 25)\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "print(\"__\" * 25)\n",
    "free_memory(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d7ae1-9063-4ae8-a5d2-4f9d4d28dec2",
   "metadata": {},
   "source": [
    "## Step 3: Approach - Synthetic Data Generation ðŸ¤–\n",
    "Synthetic data is, as the name suggests, artificial data generated to mimic real data. Typically, synthetic data is generated using sophisticated Generative AI techniques to create data similar in structure, features, and characteristics to the data found in real-world applications.\n",
    "Some key considerations when evaluating the quality of synthetic data include the randomness of the sample, how well it captures the statistical distribution of real data, and whether it includes missing or erroneous values.\n",
    "\n",
    "### Persona-driven Synthetic Data Creation\n",
    "* This work incorporates insights from [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/pdf/2406.20094).\n",
    "Previous research tends to diversify the data synthesis prompt through the following two paradigms, which are instance-driven and key-point-driven, but unfortunately, neither can practically achieve scalable synthetic data creation.\n",
    "* Following a novel persona-driven data synthesis methodology.\n",
    "The personas can be regarded as distributed carriers of world knowledge, and each individual can be associated with their unique knowledge, experience, interest, personality and profession.\n",
    "Thus, they can tap into almost every perspective encapsulated within the LLM to create diverse synthetic data at scale.\n",
    "This approach involves integrating a persona into the appropriate position in a data synthesis prompt.\n",
    "Driven by the 1 billion personas in Persona Hub, this approach can easily create diverse synthetic data at a billion scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750faf0c-c7ee-4980-8c4d-b458917d1c08",
   "metadata": {},
   "source": [
    "## Step 4: Import Packages and Create TemplateâŒ›\n",
    "\n",
    "* Import vLLM and required packages.\n",
    "* Here we are going to use a ```comedy_template``` prompt to feed the model, In the similar way you can design your own template and format the prompt template to the model using the helper functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3218ed2-7d46-4a03-a476-6864787cad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex #Include XPU namespace\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fda133-9f47-43aa-b914-bb7ac96d4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_template = '''{persona}\n",
    "\n",
    "Assume you are the persona described above and I want you to act as a stand-up comedian. Write content that reflects your unique voice, expertise, and humor, tailored to your specific field. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9eb019-3ab6-4a78-9eb1-67dc1d5a0b52",
   "metadata": {},
   "source": [
    "## Step 5: Generating Data ðŸ§ª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba9818-4602-468f-a9e7-56a533db4936",
   "metadata": {},
   "source": [
    "* Define the ```SAMPLE_SIZE```, describes how many samples of synthetic data to be generated based on the sample data you provide.\n",
    "* Select the ```MODEL```, based on your hardware capacity and VRAM.\n",
    "* Define a ```system_prompt```, ```user_prompt``` and apply the chat template by formatting input to the dataset.\n",
    "* ```Optional```: Truncating the data to avoid OOM.\n",
    "* Finally, generated data is redirected into a ```JSON``` file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2dd3e6-2aa8-4b4d-8d2d-0438ad29778e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHOICE_TEMPLATE=\"comedy\"   # template can also be  \"knowledge\" or \"math\". Feel free to try others; You can also add your customized data synthesis prompt in code/prompt_templates.py\n",
    "SAMPLE_SIZE=10  # Set sample_size=0 if you want to use the full version of 200k personas.\n",
    "OUT_PATH=f\"{CHOICE_TEMPLATE}_{SAMPLE_SIZE}_synthesis_output.jsonl\"\n",
    "MODEL_PATH=\"NousResearch/Hermes-3-Llama-3.1-8B\" # feel free to replace it with any other open-sourced LLMs supported by vllm, Ex: \"NousResearch/Nous-Hermes-llama-2-7b\".\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache() # Query for XPU(Intel GPU) and empty the cache.\n",
    "\n",
    "def request_input_format(user_prompt, tokenizer):\n",
    "    \"\"\"\n",
    "        Formating the dataset for input prompts\n",
    "        {user_prompt}: Input Prompt of the dataset\n",
    "        {tokenizer}: Tokenizer of the Model.\n",
    "        return: Formats the user_prompt according to the chat template\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are a helpful assistant.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def truncate_text(text, max_length):\n",
    "    \"\"\"\n",
    "    This function is to get rid of OOM issues, it reduces the prompt length.\n",
    "    {text}: Input prompt\n",
    "    {max_length}: customize your sequence length.\n",
    "    return: Updated Input prompt\n",
    "    \"\"\"\n",
    "    return text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "def main():\n",
    "    \"\"\"Choosing a template, run the generation with vLLM\"\"\"\n",
    "    # Load the appropriate template\n",
    "    if CHOICE_TEMPLATE == \"comedy\":\n",
    "        template = comedy_template\n",
    "    else:\n",
    "        raise ValueError(\"Invalid template type. Choose 'comedy_template', or define a custom template.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    persona_dataset = load_dataset(\"proj-persona/PersonaHub\", data_files=\"persona.jsonl\")['train']\n",
    "\n",
    "    max_char_length = 1024 #Setting a max length to data input, to avoid OOM issues.\n",
    "    persona_dataset = persona_dataset.map(lambda x: {'persona': truncate_text(x['persona'], max_char_length)})\n",
    "    \n",
    "    if SAMPLE_SIZE > 0:\n",
    "        persona_dataset = persona_dataset[:SAMPLE_SIZE]\n",
    "    print(f\"Total number of input personas: {len(persona_dataset['persona'])}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    llm = LLM(model=MODEL_PATH) # please set tensor_parallel_size based on the GPUs you are using\n",
    "\n",
    "    prompts = []\n",
    "    max_len = 2048\n",
    "\n",
    "    for persona in persona_dataset['persona']:\n",
    "        persona = persona.strip()\n",
    "        user_prompt = template.format(persona=persona)\n",
    "        prompt = request_input_format(user_prompt, tokenizer)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    print(f\"Loaded {len(prompts)} entries to process...\\n\\n\")\n",
    "    print(f\"Sample 0: {prompts[0]}\")\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=0.6, top_p=0.95, max_tokens=max_len, stop=[\"<|eot_id|>\"])\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    free_memory(llm)\n",
    "\n",
    "    with open(OUT_PATH, 'w') as out:\n",
    "        for i, output in enumerate(outputs):\n",
    "            out_txt = output.outputs[0].text\n",
    "            finish_reason = output.outputs[0].finish_reason\n",
    "            data = {'prompt': output.prompt, \"input persona\": persona_dataset['persona'][i].strip(), \"finish_reason\": finish_reason}\n",
    "            data['synthesized text'] = out_txt\n",
    "            out.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Output the results to: {OUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb23d2-ce1a-4a07-a20e-5440a4047ffb",
   "metadata": {},
   "source": [
    "## View Generated Synthetic Dataset ðŸ‘€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1ae09-4ac9-4955-aa51-c7363885dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=OUT_PATH)['train']\n",
    "# dataset = load_dataset(\"json\", data_files=\"comedy_synthesis_10.jsonl\")[\"train\"]\n",
    "print(dataset)\n",
    "print(f\"\\n\\nInput Prompt: \\n\\n{dataset[0]['prompt']}\")\n",
    "print(f\"Synthesized Text: \\n\\n{dataset[0]['synthesized text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944b2e1-8cdd-490c-aed8-134bb1c72f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Disclaimer from proj-persona/PersonaHub applies here\n",
    "\n",
    "* https://huggingface.co/datasets/proj-persona/PersonaHub\n",
    "\n",
    "PERSONA HUB can facilitate synthetic data creation at a billion-scale to simulate diverse inputs (i.e., use cases) from a wide variety of real-world users. If this data is used as input to query a target LLM to obtain its outputs at scale, there is a high risk that the LLM's knowledge, intelligence and capabilities will be dumped and easily replicated, thereby challenging the leading position of the most powerful LLMs. It is crucial to avoid misuse and ensure ethical and responsible application to prevent privacy violations and other ethical concerns.\n",
    "\n",
    "The released data is all generated by public available models (GPT-4, Llama-3 and Qwen), and is intended for research purposes only. Users also must comply with the respective license agreements and usage policies of these models when using the synthesized data. The data may contain inaccuracies, unsafe content, or biases, for which we cannot be held responsible. Please evaluate its accuracy and suitability before use. Tencent and its licensors provide the data AS-IS, without warranty of any kind, express or implied. The views and opinions expressed in the data do not necessarily reflect those of Tencent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0d2d9-3745-469d-a0cd-bb17d97e9a4c",
   "metadata": {},
   "source": [
    "### Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "For detailed information on each model's capabilities, licensing, and attribution, please refer to the respective model cards:\n",
    "\n",
    "1. **NousResearch/Hermes-3-Llama-3.1-8B**\n",
    "\n",
    "   * Model card: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B\n",
    "\n",
    "2. **NousResearch/Nous-Hermes-llama-2-7b**\n",
    "\n",
    "   * Model card: https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b\n",
    "\n",
    "3. **facebook/opt-125m**\n",
    "\n",
    "  * Model card: https://huggingface.co/facebook/opt-125m\n",
    "  \n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above. To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intelâ€™s provision of these resources does not expand or otherwise alter Intelâ€™s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdg",
   "language": "python",
   "name": "sdg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
