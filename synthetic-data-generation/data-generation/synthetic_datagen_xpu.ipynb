{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c34cccab-0a67-4186-b446-188112b95650",
   "metadata": {},
   "source": [
    "- This Notebook is created by Rajashekar Kasturi and Thasneem Vazim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f972d96",
   "metadata": {},
   "source": [
    "# ðŸ“’ Overview\n",
    "\n",
    "- Before running the notebook make sure you select the ```vllm-xpu``` environment in the kernel, If there is no such kernel present. Please follow setup instructions from [here](../vllm-setup/).\n",
    "\n",
    "- The notebook helps you create Synthetic data for **Comedy dialogue generation** ðŸ˜….\n",
    "\n",
    "- Among various prompt templates under ```prompt_templates.py```, here we've used comedy_template. In the same way you can customize your own template and import it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546189d1",
   "metadata": {},
   "source": [
    "## âŒ›Load necessary Imports\n",
    "\n",
    "* Import vLLM and required packages.\n",
    "* Import templates from the ```prompt_templates``` file, we can make necessary changes into the file and import custom templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be64ae4-d57c-47e0-bb01-eac8e2b32c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex #Include XPU namespace\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "from prompt_templates import comedy_template, instruction_template, knowledge_template # You can add your custom template under code/prompt_templates.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21822e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "* Below are few helper functions to format the user_prompt and to truncate the data to avoid OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb13ba7-5aa4-490c-ae1d-29959637bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_input_format(user_prompt, tokenizer):\n",
    "    \"\"\"\n",
    "        Formating the dataset for input prompts\n",
    "        {user_prompt}: Input Prompt of the dataset\n",
    "        {tokenizer}: Tokernizer of the Model.\n",
    "        return: Formats the user_prompt according to the chat template\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are a helpful assistant.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def truncate_text(text, max_length):\n",
    "    \"\"\"\n",
    "    This function is to get rid of OOM issues, it reduces the prompt length.\n",
    "    {text}: Input prompt\n",
    "    {max_length}: Lenght to want to slice.\n",
    "    return: Updated Input prompt\n",
    "    \"\"\"\n",
    "    return text[:max_length] if len(text) > max_length else text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7b3df-c88e-4790-ba74-c430e250ce18",
   "metadata": {},
   "source": [
    "## ðŸ§ª Choose the template and Run generation\n",
    "\n",
    "* Select a template from the ```prompt_templates.py```\n",
    "\n",
    "* Define the sample size, which descibes how many samples of synthetic data to be generated based on the whole dataset.\n",
    "\n",
    "* Select the model, based on your hardware capacity and VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d9753-0ab4-40ec-9e61-7f17f85b3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOICE_TEMPLATE=\"comedy\"   # template can also be  \"knowledge\" or \"math\". Feel free to try others; You can also add your customized data synthesis prompt in code/prompt_templates.py\n",
    "SAMPLE_SIZE=10  # Set sample_size=0 if you want to use the full version of 200k personas.\n",
    "OUT_PATH=f\"{CHOICE_TEMPLATE}_{SAMPLE_SIZE}_synthesis_output.jsonl\"\n",
    "MODEL_PATH=\"meta-llama/Meta-Llama-3-70B\" # feel free to replace it with any other open-sourced LLMs supported by vllm, Ex: \"meta-llama/Meta-Llama-3-8B-Instruct\".\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache() # Query for XPU(Intel GPU) and empty the cache.\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Load the appropriate template\n",
    "    if CHOICE_TEMPLATE == \"instruction\":\n",
    "        template = instruction_template\n",
    "    elif CHOICE_TEMPLATE == \"knowledge\":\n",
    "        template = knowledge_template\n",
    "    elif CHOICE_TEMPLATE == \"comedy\":\n",
    "        template = comedy_template\n",
    "    else:\n",
    "        raise ValueError(\"Invalid template type. Choose from 'instruction', 'knowledge', 'math' or 'npc', or create a custom template and add to the imports.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    persona_dataset = load_dataset(\"proj-persona/PersonaHub\", data_files=\"persona.jsonl\")['train']\n",
    "\n",
    "    max_char_length = 1024 #Setting a max length to data input, to avoid OOM issues.\n",
    "    persona_dataset = persona_dataset.map(lambda x: {'persona': truncate_text(x['persona'], max_char_length)})\n",
    "    \n",
    "    if SAMPLE_SIZE > 0:\n",
    "        persona_dataset = persona_dataset[:SAMPLE_SIZE]\n",
    "    print(f\"Total number of input personas: {len(persona_dataset['persona'])}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    llm = LLM(model=MODEL_PATH) # please set tensor_parallel_size based on the GPUs you are using\n",
    "\n",
    "    prompts = []\n",
    "    max_len = 2048\n",
    "\n",
    "    for persona in persona_dataset['persona']:\n",
    "        persona = persona.strip()\n",
    "        user_prompt = template.format(persona=persona)\n",
    "        prompt = request_input_format(user_prompt, tokenizer)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    print(f\"Loaded {len(prompts)} entries to process...\\n\\n\")\n",
    "    print(f\"Sample 0: {prompts[0]}\")\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=0.6, top_p=0.95, max_tokens=max_len, stop=[\"<|eot_id|>\"])\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    with open(OUT_PATH, 'w') as out:\n",
    "        for i, output in enumerate(outputs):\n",
    "            out_txt = output.outputs[0].text\n",
    "            finish_reason = output.outputs[0].finish_reason\n",
    "            data = {'prompt': output.prompt, \"input persona\": persona_dataset['persona'][i].strip(), \"finish_reason\": finish_reason}\n",
    "            data['synthesized text'] = out_txt\n",
    "            out.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Outputted the results to: {OUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c2778-c030-4db4-93bf-a213982a7003",
   "metadata": {},
   "source": [
    "## ðŸ‘€ View the generated Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61c1d6-9e3a-45ba-95b8-f1f7385e711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=OUT_PATH)['train']\n",
    "# dataset = load_dataset(\"json\", data_files=\"comedy_synthesis_10.jsonl\")[\"train\"]\n",
    "print(dataset)\n",
    "print(f\"\\n\\nInput Prompt: \\n\\n{dataset[0]['prompt']}\")\n",
    "print(f\"Synthesized Text: \\n\\n{dataset[0]['synthesized text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
