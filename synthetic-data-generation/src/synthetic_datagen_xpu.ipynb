{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c34cccab-0a67-4186-b446-188112b95650",
   "metadata": {},
   "source": [
    "- This Notebook is created by Rajashekar Kasturi and Thasneem Vazim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b43be-35cf-49c6-be33-409ff4a76311",
   "metadata": {},
   "source": [
    "## Load necessary imports for vLLM and Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb13ba7-5aa4-490c-ae1d-29959637bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ue5c01becfd98f8b9e769a8179f45cdf/.conda/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-07 11:39:10 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 11:39:12,914\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from prompt_templates import comedy_template, instruction_template, knowledge_template # You can add your custom template under code/prompt_templates.py\n",
    "\n",
    "def request_input_format(user_prompt, tokenizer):\n",
    "    \"\"\"\n",
    "        Formating the dataset for input prompts\n",
    "        user_prompt: Input Prompt of the dataset\n",
    "        tokenizer:\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are a helpful assistant.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def truncate_text(text, max_length):\n",
    "    \"\"\"\n",
    "    This function is to get rid of OOM issues, it reduces the prompt length.\n",
    "    \"\"\"\n",
    "    return text[:max_length] if len(text) > max_length else text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7b3df-c88e-4790-ba74-c430e250ce18",
   "metadata": {},
   "source": [
    "## Load reference dataset and Template for Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68d9753-0ab4-40ec-9e61-7f17f85b3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to create Level Zero tracer: 2013265921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input personas: 50\n",
      "WARNING 11-07 11:39:18 config.py:376] Async output processing is only supported for CUDA or TPU. Disabling it for other platforms.\n",
      "INFO 11-07 11:39:18 llm_engine.py:226] Initializing an LLM engine (v0.6.3.dev0+g7193774b.d20241009) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=xpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-07 11:39:19 _logger.py:68] bfloat16 is not fully supported on XPU, casting to float16.\n",
      "WARNING 11-07 11:39:19 _logger.py:68] CUDA graph is not supported on XPU, fallback to the eager mode.\n",
      "observability_config is ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False)\n",
      "INFO 11-07 11:39:19 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\n",
      "INFO 11-07 11:39:19 selector.py:138] Using IPEX attention backend.\n",
      "INFO 11-07 11:39:19 selector.py:193] Cannot use _Backend.FLASH_ATTN backend on XPU.\n",
      "INFO 11-07 11:39:19 selector.py:138] Using IPEX attention backend.\n",
      "INFO 11-07 11:39:20 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.09s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.83s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.55s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-07 11:39:33 utils.py:747] Pin memory is not supported on XPU.\n",
      "INFO 11-07 11:39:33 xpu_model_runner.py:406] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 11:39:36 gpu_executor.py:122] # GPU blocks: 14459, # CPU blocks: 2048\n",
      "Loaded 50 entries to process...\n",
      "\n",
      "\n",
      "Sample 0: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A Political Analyst specialized in El Salvador's political landscape.\n",
      " \n",
      "Assume you are the persona described above and I want you to act as a stand-up comedian. Write content that reflects your unique voice, expertise, and humor, tailored to your specific field.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, est. speed input: 92.39 toks/s, output: 643.91 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputted the results to: comedy_50_synthesis_output.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CHOICE_TEMPLATE=\"comedy\"   # template can also be  \"knowledge\" or \"math\". Feel free to try others; You can also add your customized data synthesis prompt in code/prompt_templates.py\n",
    "SAMPLE_SIZE=50  # Set sample_size=0 if you want to use the full version of 200k personas.\n",
    "OUT_PATH=f\"{CHOICE_TEMPLATE}_{SAMPLE_SIZE}_synthesis_output.jsonl\"\n",
    "MODEL_PATH=\"meta-llama/Meta-Llama-3-70B\" # feel free to replace it with any other open-sourced LLMs supported by vllm, Ex: \"meta-llama/Meta-Llama-3-8B-Instruct\".\n",
    "\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex #Include XPU namespace\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache() # Query for XPU(Intel GPU) and empty the cache.\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Load the appropriate template\n",
    "    if CHOICE_TEMPLATE == \"instruction\":\n",
    "        template = instruction_template\n",
    "    elif CHOICE_TEMPLATE == \"knowledge\":\n",
    "        template = knowledge_template\n",
    "    elif CHOICE_TEMPLATE == \"comedy\":\n",
    "        template = comedy_template\n",
    "    else:\n",
    "        raise ValueError(\"Invalid template type. Choose from 'instruction', 'knowledge', 'math' or 'npc', or create a custom template and add to the imports.\")\n",
    "\n",
    "    # Load the dataset\n",
    "    persona_dataset = load_dataset(\"proj-persona/PersonaHub\", data_files=\"persona.jsonl\")['train']\n",
    "\n",
    "    max_char_length = 1024 #Setting a max length to data input, to avoid OOM issues.\n",
    "    persona_dataset = persona_dataset.map(lambda x: {'persona': truncate_text(x['persona'], max_char_length)})\n",
    "    \n",
    "    if SAMPLE_SIZE > 0:\n",
    "        persona_dataset = persona_dataset[:SAMPLE_SIZE]\n",
    "    print(f\"Total number of input personas: {len(persona_dataset['persona'])}\")\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    llm = LLM(model=MODEL_PATH) # please set tensor_parallel_size based on the GPUs you are using\n",
    "\n",
    "    prompts = []\n",
    "    max_len = 2048\n",
    "    \n",
    "\n",
    "    for persona in persona_dataset['persona']:\n",
    "        persona = persona.strip()\n",
    "        user_prompt = template.format(persona=persona)\n",
    "        prompt = request_input_format(user_prompt, tokenizer)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    print(f\"Loaded {len(prompts)} entries to process...\\n\\n\")\n",
    "    print(f\"Sample 0: {prompts[0]}\")\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=0.6, top_p=0.95, max_tokens=max_len, stop=[\"<|eot_id|>\"])\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    with open(OUT_PATH, 'w') as out:\n",
    "        for i, output in enumerate(outputs):\n",
    "            out_txt = output.outputs[0].text\n",
    "            finish_reason = output.outputs[0].finish_reason\n",
    "            data = {'prompt': output.prompt, \"input persona\": persona_dataset['persona'][i].strip(), \"finish_reason\": finish_reason}\n",
    "            data['synthesized text'] = out_txt\n",
    "            out.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Outputted the results to: {OUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c2778-c030-4db4-93bf-a213982a7003",
   "metadata": {},
   "source": [
    "## Load Generated Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a61c1d6-9e3a-45ba-95b8-f1f7385e711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 50 examples [00:00, 9131.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'input persona', 'finish_reason', 'synthesized text'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Input Prompt: \n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "A Political Analyst specialized in El Salvador's political landscape.\n",
      " \n",
      "Assume you are the persona described above and I want you to act as a stand-up comedian. Write content that reflects your unique voice, expertise, and humor, tailored to your specific field.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Synthesized Text: \n",
      "\n",
      "(clears throat) Ah, ¡Hola, amigos! So, I know what you're thinking: \"A political analyst as a stand-up comedian? ¡Eso es una locura!\" But trust me, I've got the credentials – I've spent years studying the intricacies of El Salvador's political landscape, and I've got jokes for days! (winks)\n",
      "\n",
      "So, have you heard about the latest developments in the FMLN party? It's like a soap opera! (chuckles) \"Who's in, who's out, who's got the most Twitter followers?\" (audience laughs) I mean, seriously, it's like trying to keep track of who's got the most machetes in their family. (laughter)\n",
      "\n",
      "And don't even get me started on the ARENA party. They're like the Kardashians of politics – always drama, always scandals, always trying to out-do each other in a battle for relevance. (audience laughs) I mean, have you seen their social media? It's like a never-ending feud between Nayib Bukele and Norman Quiñones. (mimics argumentative tone) \"You're a puppet of the US!\" \"No, you're a puppet of the US!\" (audience laughs)\n",
      "\n",
      "And then there's the Farabundo Martí National Liberation Front (FMLN). Ah, the FMLN. They're like the grumpy old uncles of El Salvadoran politics. Always complaining about the government, always predicting doom and gloom. (in a gruff voice) \"The system is broken, the system is corrupt, we need to overthrow the system!\" (audience laughs) You know, I'm like, \"Dude, I get it, the system is messed up, but can you at least try to make a decent cup of coffee?\" (laughter)\n",
      "\n",
      "But in all seriousness, folks, El Salvador is a country that's made tremendous progress in recent years. We've got a new government that's actually trying to make a difference, and we've got a vibrant civil society that's demanding change. (smiling) And let's be real, we've got some of the best pupusas in the world – that's worth fighting for! (audience laughs and applauds)\n",
      "\n",
      "So, that's my take on El Salvador's political landscape. It's a wild ride, but hey, at least we've got each other – and our pupusas. (winks) ¡Viva El Salvador! (applause)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=OUT_PATH)['train']\n",
    "# dataset = load_dataset(\"json\", data_files=\"comedy_synthesis_15000.jsonl\")[\"train\"]\n",
    "print(dataset)\n",
    "print(f\"Input Prompt: \\n\\n{dataset[0]['prompt']}\")\n",
    "print(f\"Synthesized Text: \\n\\n{dataset[0]['synthesized text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
