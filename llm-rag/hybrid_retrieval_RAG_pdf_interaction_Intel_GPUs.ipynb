{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f87c7ec",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0 Copyright (c) 2023, Aryan Karumuri srirajx.aryan.karumuri@intel.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42843b-10d1-4149-b88f-b9db63dd61b3",
   "metadata": {},
   "source": [
    "# PDF Interaction with Hybrid Retrievals üìÑüîç\n",
    "\n",
    "Welcome to the future of document interaction! Whether you're a student, researcher, developer, or simply someone who works with PDFs, this guide will show you how to make the most of advanced retrieval techniques to chat with your PDFs.\n",
    "\n",
    "Have you ever wished you could ask questions about a PDF document and get quick, relevant answers? With **Hybrid Retrievals**, **Conceptualization**, **Reranking**, and **Contextual Compression**, you can enhance your PDF experience like never before.\n",
    "\n",
    "Choose your PDF, and the system will analyze the content using both traditional keyword search and advanced machine learning models. This hybrid approach allows you to query the document, extract information, and even have a back-and-forth conversation about its contents.\n",
    "\n",
    "This tool is powered by cutting-edge AI technology, ensuring high-quality results and accurate, context-aware responses. No complex setup, just easy interaction with your documents in real time.\n",
    "\n",
    "Ready to unlock the potential of your PDFs and have intelligent conversations with them? Let‚Äôs dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5bdcf-6913-4c57-92b3-dff882f3d179",
   "metadata": {},
   "source": [
    "# Setup & Imports üöÄ\n",
    "\n",
    "### Setup\n",
    "\n",
    "1. Run the below to create a new virtual environment.  \n",
    "2. To select the new environment/kernel in Jupyter, choose **\"Python (llm_rag_venv)\"** from the dropdown menu in the top-right corner of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fe827-87ff-4bb3-8d4b-0c3d431030c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Python virtual environment\n",
    "!python3 -m venv llm_rag_venv\n",
    "\n",
    "# Step 2: Install dependencies (torch, transformers, etc.) in the virtual environment\n",
    "# Since we can't activate the environment directly in Jupyter, we'll use the full path to `pip` inside the venv.\n",
    "!llm_rag_venv/bin/pip install torch==2.3.1+cxx11.abi \\\n",
    "                             torchvision==0.18.1+cxx11.abi \\\n",
    "                             torchaudio==2.3.1+cxx11.abi \\\n",
    "                             oneccl_bind_pt==2.3.100+xpu \\\n",
    "                             intel-extension-for-pytorch==2.3.110+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n",
    "\n",
    "!llm_rag_venv/bin/pip install transformers==4.44.2 \\\n",
    "                             langchain==0.3.3 \\\n",
    "                             langchain-community==0.3.0 \\\n",
    "                             langchain-core==0.3.10 \\\n",
    "                             langchain-huggingface==0.1.0 \\\n",
    "                             langchain-text-splitters==0.3.0 \\\n",
    "                             langsmith==0.1.128 \\\n",
    "                             InstructorEmbedding==1.0.1 \\\n",
    "                             huggingface-hub==0.25.1 \\\n",
    "                             chroma-hnswlib==0.7.6 \\\n",
    "                             chromadb==0.5.11 \\\n",
    "                             accelerate==1.0.1 \\\n",
    "                             pypdf \\\n",
    "                             ipywidgets \\\n",
    "                             rank-bm25==0.2.2\n",
    "\n",
    "!llm_rag_venv/bin/pip install sentence-transformers==2.2.2\n",
    "\n",
    "# Step 3: Install `ipykernel` to register the virtual environment as a Jupyter kernel\n",
    "!llm_rag_venv/bin/pip install ipykernel\n",
    "\n",
    "# Step 4: Register the virtual environment as a Jupyter kernel\n",
    "!llm_rag_venv/bin/python -m ipykernel install --user --name=llm_rag_venv --display-name \"Python (llm_rag_venv)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f7f47-447e-46f9-9e23-9e1edb7ff2b5",
   "metadata": {},
   "source": [
    "### Imports üì¶\n",
    "\n",
    "In this section, we import all the necessary libraries and modules required for various tasks like document loading, embeddings, model generation, and chaining.\n",
    "\n",
    "### Key Libraries:\n",
    "- **PyTorch**: Used for deep learning model operations, and device management (CPU or GPU).\n",
    "- **LangChain**: A framework to work with language models, document processing, embeddings, and chains of operations.\n",
    "- **Transformers**: Hugging Face's `transformers` library is used to work with pretrained models for text generation and embedding extraction.\n",
    "- **Chroma**: A library for creating and managing vector databases, storing embeddings for document retrieval.\n",
    "\n",
    "Additionally, we suppress any warnings that might arise from model loading and setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4d29c-ed1a-4923-b1d6-4b9417cc3650",
   "metadata": {},
   "source": [
    "## Now select the kerenel as 'Python (llm_rag_venv)' in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125541e8-e6b8-4e16-a0f7-8b525bfaa19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import langchain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "\n",
    "# Langchain related imports\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.cache import InMemoryCache\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Chroma and related imports\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "\n",
    "# Cross-encoder related imports\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92b03b-3a75-4130-bd62-95608578ecac",
   "metadata": {},
   "source": [
    "## Device Setup üñ•Ô∏èüîß\n",
    "\n",
    "This section checks if a compatible GPU (xPU) is available for use. If so, the code sets the device to `xPU` for accelerated computations. If no GPU is found, it defaults to using the CPU.\n",
    "\n",
    "### Key Components:\n",
    "- **Device Selection**: The device is set based on availability. If the `xPU` device is available (for Intel GPUs), it will be used. Otherwise, the system defaults to CPU.\n",
    "- **Memory Management**: If using xPU, the memory is cleared to ensure there is no cached memory from previous operations, optimizing the resources for the next computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e60ef7-d401-4acf-9131-59b106e5a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: Intel(R) Data Center GPU Max 1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Failed to create Level Zero tracer: 2013265921\n"
     ]
    }
   ],
   "source": [
    "# Check and set the device (either XPU or CPU)\n",
    "device = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "\n",
    "# If XPU is available, use it, otherwise fall back to CPU\n",
    "if device.type == \"xpu\":\n",
    "    # Empty the XPU cache\n",
    "    torch.xpu.empty_cache()\n",
    "    print(f\"Using device: {torch.xpu.get_device_name()}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b62c46-53ca-459e-b15a-64b1e8fd90af",
   "metadata": {},
   "source": [
    "## Paths Configuration üìÇ\n",
    "\n",
    "This section sets up the paths for various directories used in the project. These paths are essential for file management, including loading documents, storing embeddings, and saving the final database.\n",
    "\n",
    "### Key Directories:\n",
    "- `ROOT_DIRECTORY`: The base directory of the project where all source code and notebooks reside.\n",
    "- `SOURCE_DIRECTORY`: Directory containing source documents (e.g., PDFs) for processing.\n",
    "- `PERSIST_DIRECTORY`: Directory where embeddings and other persisted data will be stored.\n",
    "- `pdf_file_path`: Path to the specific PDF file that will be processed.\n",
    "\n",
    "A **LocalFileStore** object is also created here to manage embedding caching.\n",
    "\n",
    "**Note**: By default, \"attention.pdf\" is selected. If one wants to select other then they need to run cell by cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3aa236-562f-417d-826e-c829348533e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root and source directories\n",
    "ROOT_DIRECTORY = Path.cwd()\n",
    "SOURCE_DIRECTORY = ROOT_DIRECTORY / 'SOURCE_DOCUMENTS'\n",
    "PERSIST_DIRECTORY = ROOT_DIRECTORY / 'DB'  # Path to the DB directory\n",
    "\n",
    "# Local file store for caching\n",
    "store = LocalFileStore(\"./db_cache/cache/\")\n",
    "\n",
    "# Remove the PERSIST_DIRECTORY if it already exists\n",
    "if PERSIST_DIRECTORY.exists():\n",
    "    os.system(f'rm -rf {PERSIST_DIRECTORY}')\n",
    "\n",
    "# Ensure the PERSIST_DIRECTORY exists (create it if it doesn't)\n",
    "PERSIST_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# You can now confirm the directory was created\n",
    "print(f\"PERSIST_DIRECTORY exists at: {PERSIST_DIRECTORY}\")\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=True,\n",
    ")\n",
    "\n",
    "# List of PDF filenames\n",
    "files = os.listdir(str(ROOT_DIRECTORY)+'/data/')\n",
    "pdf_files=[]\n",
    "for file in files:\n",
    "    if  file[-3:]=='pdf':\n",
    "        pdf_files.append(file)\n",
    "\n",
    "# Create the full paths for the PDFs\n",
    "pdf_paths = [ROOT_DIRECTORY / 'data' / pdf for pdf in pdf_files]\n",
    "\n",
    "# Create a dropdown widget for selecting a PDF\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=pdf_files,\n",
    "    description='Select PDF:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Define a function to handle the dropdown change\n",
    "selected_pdf = None\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    global selected_pdf, selected_pdf_path\n",
    "    selected_pdf = change.new    \n",
    "    selected_pdf_path = ROOT_DIRECTORY / 'data' / selected_pdf \n",
    "\n",
    "if selected_pdf is None:\n",
    "    selected_pdf = \"attention.pdf\"\n",
    "    selected_pdf_path = ROOT_DIRECTORY / 'data' / selected_pdf     \n",
    "        \n",
    "\n",
    "# Attach the function to the dropdown's 'value' change event\n",
    "dropdown.observe(on_dropdown_change, names='value')\n",
    "\n",
    "# Display the dropdown\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f7b48-c81d-4fed-af77-34e208a5c50c",
   "metadata": {},
   "source": [
    "## Configuration and Session Management ‚öôÔ∏èüìÖ\n",
    "\n",
    "### Embedding Model:\n",
    "- **EMBEDDING_MODEL_NAME**: Specifies the embedding model used for generating document and query embeddings. Here, `\"hkunlp/instructor-large\"` is used, which is optimized for instruction-based embeddings suitable for document retrieval tasks.\n",
    "\n",
    "### Hugging Face Model:\n",
    "- **MODEL_ID**: Identifies the language model for text generation. `\"NousResearch/Llama-2-7b-chat-hf\"` is a conversational model designed for chat-based interactions, used to generate responses based on user queries.\n",
    "\n",
    "### Generation Config:\n",
    "- **MAX_LENGTH**: Maximum tokens allowed in the model's response (4096 tokens).\n",
    "- **TEMPERATURE**: Controls randomness in generation; set to `0.1` for deterministic outputs.\n",
    "- **REPETITION_PENALTY**: Discourages repeated phrases, set to `1.15`.\n",
    "\n",
    "### Session State Management:\n",
    "- **session_store**: A dictionary that stores chat history for each session.\n",
    "- **session_id**: A default session ID (`\"default_session\"`) is used.\n",
    "- **ChatMessageHistory**: Manages the chat history, ensuring that past interactions are accessible for contextual responses in ongoing conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77176229-0a03-4d97-9b10-23552eba0347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
    "\n",
    "# Hugging Face Model\n",
    "MODEL_ID = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Generation config parameters\n",
    "MAX_LENGTH = 4096\n",
    "TEMPERATURE = 0.1\n",
    "REPETITION_PENALTY = 1.15\n",
    "\n",
    "# Session state management\n",
    "session_store = {}\n",
    "session_id = \"default_session\"\n",
    "\n",
    "if session_id not in session_store:\n",
    "    session_store[session_id] = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05edea6d-9737-47e7-8c13-daa5a9d4946b",
   "metadata": {},
   "source": [
    "## Load & Chunk Documents üì•üìë\n",
    "\n",
    "This section deals with loading a PDF file and splitting its content into manageable chunks for processing.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Loading the PDF**: The `PyPDFLoader` is used to load the document from the specified `pdf_file_path`.\n",
    "2. **Splitting the Document**: The document is split into chunks using `RecursiveCharacterTextSplitter`. This is done to break the document into smaller sections, allowing more efficient processing.\n",
    "3. **Filtering Chunks**: After splitting, the chunks are filtered to remove any empty content before embedding.\n",
    "\n",
    "This process is necessary to prepare the document content for embedding and retrieval in a vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fc2df7-a2ff-40dc-9fd5-0071eb80b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the PDF\n",
    "loader = PyPDFLoader(selected_pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(\"No documents loaded from the PDF.\")\n",
    "\n",
    "# Splitting the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=240, chunk_overlap=50)\n",
    "document_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Creating documents from the chunks\n",
    "documents = [\n",
    "    Document(page_content=chunk.page_content) \n",
    "    for chunk in document_chunks if chunk.page_content\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838f8c4-b686-4aa4-a993-1b98e7865a22",
   "metadata": {},
   "source": [
    "## Getting Embeddings & Embedder Setup üß†üîó\n",
    "\n",
    "### `get_embeddings(device_type)`:\n",
    "- This function selects the appropriate embedding model based on the value of `EMBEDDING_MODEL_NAME`.\n",
    "    - If the model name contains \"instructor\" (indicating an instruction-based embedding model like `hkunlp/instructor-large`), it initializes the `HuggingFaceInstructEmbeddings` class, which generates embeddings for both documents and queries with custom instructions.\n",
    "    - If the model is not instruction-based, the function defaults to using `HuggingFaceEmbeddings`, a general-purpose embedding model.\n",
    "\n",
    "### Getting Embeddings:\n",
    "- The `get_embeddings(device)` function is called to retrieve the appropriate embedding model based on the selected `device_type` (e.g., CPU or GPU).\n",
    "\n",
    "### Cache-Backed Embeddings:\n",
    "- **`CacheBackedEmbeddings`**: This class wraps around the selected embedding model to provide caching support. It ensures that embeddings are stored efficiently and can be quickly retrieved from the cache, reducing the need for redundant computations.\n",
    "- **`from_bytes_store`**: This method loads the embeddings from the cache store, specified by `store`, and associates them with the provided namespace (`EMBEDDING_MODEL_NAME`), enabling faster access to embeddings during document retrieval tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b993c35-c41b-4f0f-a209-619c4e7101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings\n",
    "def get_embeddings(device_type):\n",
    "    if \"instructor\" in EMBEDDING_MODEL_NAME:\n",
    "        return HuggingFaceInstructEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            model_kwargs={\"device\": device_type},\n",
    "            embed_instruction=\"Represent the document for retrieval:\",\n",
    "            query_instruction=\"Represent the question for retrieving supporting documents:\"\n",
    "        )\n",
    "    else:\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            model_kwargs={\"device\": device_type}\n",
    "        )\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = get_embeddings(device)\n",
    "\n",
    "# Create embedder with cache-backed embeddings\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(embeddings, store, namespace=EMBEDDING_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710699ff-d5c3-4413-8154-3b5041c4c167",
   "metadata": {},
   "source": [
    "## Database Creation with Chroma üóÉÔ∏è\n",
    "\n",
    "### `Chroma.from_documents()`:\n",
    "- This function creates a vector database from the provided documents, using embeddings generated by the `embedder`.\n",
    "    - **`documents`**: A list of documents that will be embedded and stored in the database.\n",
    "    - **`embedder`**: The embedding model used to generate embeddings for the documents.\n",
    "    - **`persist_directory`**: Specifies the directory where the vector database will be stored persistently. The `PERSIST_DIRECTORY` path is converted to a string to ensure compatibility.\n",
    "    - **`client_settings`**: Configuration settings for the Chroma client, typically specifying things like connection settings or storage options.\n",
    "\n",
    "The `Chroma` database enables efficient vector search and retrieval, where documents are indexed by their embeddings for fast querying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b01ee13-6337-4728-afe6-4b8b9e06d6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DB creation\n",
    "db = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedder,\n",
    "    persist_directory=str(PERSIST_DIRECTORY),\n",
    "    client_settings=CHROMA_SETTINGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c1f21-bf66-4200-92b0-ab4051f0da6c",
   "metadata": {},
   "source": [
    "## Hybrid Retriever Setup üîç\n",
    "In this section, we set up a hybrid retriever that combines multiple retrieval methods to answer user queries effectively.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **DB Retriever**: \n",
    "    - Uses the Chroma vector database (`db`) for efficient vector-based retrieval, leveraging the embeddings stored in the database for document search.\n",
    "    \n",
    "- **BM25 Retriever**:\n",
    "    - A classical information retrieval technique based on the BM25 algorithm, which ranks documents according to their relevance to the query. The `k=5` limits the number of retrieved documents to 5.\n",
    "    \n",
    "- **Contextual Compression Retriever**:\n",
    "    - The **Contextual Compression Retriever** is an advanced retrieval technique that combines the power of deep learning models with traditional retrieval methods. It works by first retrieving documents using an initial retrieval method (e.g., BM25 or Chroma database). \n",
    "    - **Purpose**: The primary goal of the Contextual Compression Retriever is to enhance the relevance of the returned documents by considering the **context of the query**. This is crucial in handling complex queries where context and intent are more important than just keyword matches. By reranking, it selects the most relevant documents that best align with the user's intent and query context.\n",
    "    - **Key Feature**: The model chooses the **top 3 documents** after reranking, ensuring that the most relevant results are provided to the user.\n",
    "\n",
    "- **Reranking**:\n",
    "    - **Reranking** refers to the process where the documents retrieved by an initial retrieval method (e.g., BM25) are passed through a more sophisticated model to score and reorder them. This step helps refine the relevance of the documents by considering not only keyword matches but also the semantic context of both the query and the documents.\n",
    "    - **How It Works**: The reranking model (e.g., `BAAI/bge-reranker-base`) processes the query and documents through a transformer-based model that understands deeper semantic relationships. It evaluates how well each document answers the user's question in the given context and reorders them accordingly. The final ranking reflects the documents that are the most relevant based on the **contextual meaning** of the query, rather than just matching keywords.\n",
    "    - **Purpose**: The goal of reranking is to improve the quality of the retrieved documents by applying a more **context-aware model** that ensures the documents returned are not only relevant based on keywords but also semantically aligned with the user's query.\n",
    "\n",
    "- **Ensemble Retriever**:\n",
    "    - Combines the BM25 and Contextual Compression Retrievers into an ensemble. The retrievers contribute proportionally to the final results (weights=[0.7, 0.3]), enabling the hybrid retriever to leverage both classical and modern retrieval methods. This balance ensures that the system delivers comprehensive document coverage while maintaining contextual accuracy in its results.\n",
    "\n",
    "### Expected Output:\n",
    "The ensemble retriever returns the most relevant documents by leveraging both traditional and advanced retrieval methods, combining the strengths of keyword-based ranking (BM25) and context-aware reranking. Reranking ensures that the documents returned are not only relevant based on keywords but also semantically aligned with the user's query, leading to more precise and contextually appropriate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa46558-770b-4095-af33-d5c18fedc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrievers():\n",
    "    # DB Retriever\n",
    "    db_retriever = db.as_retriever()\n",
    "    \n",
    "    # BM25 Retriever\n",
    "    sparse_retriever = BM25Retriever.from_documents(documents)\n",
    "    sparse_retriever.k = 5\n",
    "\n",
    "    # Adding Contextual Compression Retriever\n",
    "    model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "    compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=db_retriever\n",
    "    )\n",
    "\n",
    "    # Creating Ensemble Retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[compression_retriever, sparse_retriever],\n",
    "        weights=[0.7, 0.3]\n",
    "    )\n",
    "    \n",
    "    return ensemble_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08de5d-ee2e-4cc8-8b0d-033d19a4d3fd",
   "metadata": {},
   "source": [
    "## LLM Setup ü§ñüîß\n",
    "\n",
    "This function sets up a text generation pipeline using Hugging Face's transformer model, with integration into LangChain for enhanced capabilities.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Model and Tokenizer Setup**:\n",
    "    - Loads the pre-trained language model (`MODEL_ID`) and tokenizer for tokenizing text and generating outputs.\n",
    "    - The model uses `AutoModelForCausalLM` and is set to use `bfloat16` precision for efficient memory usage.\n",
    "\n",
    "- **Generation Configuration**:\n",
    "    - Loads model-specific generation settings (e.g., `max_length`, `temperature`, `repetition_penalty`) to control the behavior of the generated text.\n",
    "\n",
    "- **Text Generation Pipeline**:\n",
    "    - A Hugging Face pipeline is set up for text generation with the model and tokenizer.\n",
    "    - Configured with parameters like `max_length`, `temperature`, and `num_return_sequences` to guide output generation.\n",
    "\n",
    "- **LangChain Cache**:\n",
    "    - An in-memory cache (`InMemoryCache`) is set up for LangChain's LLM to store responses efficiently.\n",
    "\n",
    "- **Return Hugging Face Pipeline**:\n",
    "    - The Hugging Face pipeline is wrapped in LangChain‚Äôs `HuggingFacePipeline` for seamless integration and use in advanced tasks like retrieval-augmented generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0152dabc-eaeb-482b-a47c-de32acc7f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm():\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "    # Load the generation configuration for the model\n",
    "    generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
    "\n",
    "    # Create a text generation pipeline\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        device=device,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=MAX_LENGTH,\n",
    "        temperature=TEMPERATURE,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        generation_config=generation_config,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Set up in-memory cache for LangChain's LLM\n",
    "    langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "    # Return the HuggingFace pipeline wrapped in LangChain's HuggingFacePipeline\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b08b9-3eed-4732-8650-ea92827a5cc9",
   "metadata": {},
   "source": [
    "## History-Aware Retriever üìúüîç\n",
    "\n",
    "This section defines a **history-aware retriever** that processes chat history to refine the user's question and retrieve the most relevant documents or answers.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Contextualize User Question**:\n",
    "    - A system prompt is defined to formulate a standalone question from the chat history and the user's latest query. \n",
    "    - The prompt ensures that the user's query can be understood without needing the entire history, focusing on reformulating the question if needed.\n",
    "\n",
    "- **Chat Prompt Template**:\n",
    "    - The `ChatPromptTemplate` is created using the system prompt, chat history, and user input. It enables the model to interpret the context and refine the query for better retrieval.\n",
    "\n",
    "- **History-Aware Retriever**:\n",
    "    - The history-aware retriever is created by passing the **language model** (`llm()`), **hybrid retriever** (`hybrid_retrievers()`), and **contextualize question prompt** (`contextualize_q_prompt`) to the `create_history_aware_retriever` function. This combination allows the retriever to understand the context from previous messages and adjust the retrieval process accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a388b1e3-f6ea-423a-b1c1-944f1bf4694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_aware_retriever():\n",
    "    # Define the system prompt to contextualize the user question\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question, \"\n",
    "        \"which might or mightnot reference context in the chat history, \"\n",
    "        \"check the query carefully and formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "\n",
    "    # Create the chat prompt using the system prompt and user input\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the history-aware retriever\n",
    "    history_retriever = create_history_aware_retriever(\n",
    "        llm(), \n",
    "        hybrid_retrievers(), \n",
    "        contextualize_q_prompt\n",
    "    )\n",
    "    \n",
    "    return history_retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7c116-82bf-4119-8a65-c7e6978f8bc2",
   "metadata": {},
   "source": [
    "## Question Answering (QA) Prompt Setup ‚ùìüí¨\n",
    "\n",
    "This section defines the prompt template used to interact with the language model for question-answering tasks.\n",
    "\n",
    "### Key Components:\n",
    "- **System Prompt**: A system-level prompt defines the task for the assistant: to answer questions strictly using the provided context.\n",
    "- **Chat Prompt Template**: The `ChatPromptTemplate` is used to manage user input and chat history to generate context-sensitive answers.\n",
    "\n",
    "This is used to format the interaction with the language model in a structured manner, ensuring that the model responds only with contextually relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9c5dd1-f712-41dc-8c22-239bfbee3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_prompt():\n",
    "    # Define the system prompt for efficient question-answering\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that answers questions based strictly on the provided context. \"\n",
    "        \"If the answer is not clear or not present in the context, respond with 'I don't know'. \"\n",
    "        \"Do not provide any additional information beyond the context provided. \"\n",
    "        \"Use the context below to answer the user's question:\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    # Create the QA prompt with improved instructions\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return qa_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0a6e-22da-433d-b913-6fd34adbe93d",
   "metadata": {},
   "source": [
    "## Session History Management üóÇÔ∏èüîí\n",
    "\n",
    "This function manages the chat history for a given session. It stores and retrieves chat messages to ensure that the conversation history is maintained.\n",
    "\n",
    "### Key Functionality:\n",
    "1. **Session History**: A dictionary (`session_store`) is used to store the chat history for each session.\n",
    "2. **History Retrieval**: The `get_session_history()` function retrieves the conversation history for a specific session (either existing or default).\n",
    "\n",
    "This ensures that the assistant can recall previous interactions and provide contextually relevant answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2617014d-056e-4386-9144-7d335d0e4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session: str) -> ChatMessageHistory:\n",
    "    # Check if the session exists, if not, create a new one\n",
    "    if session not in session_store:\n",
    "        session_store[session] = ChatMessageHistory()\n",
    "    \n",
    "    # Return the chat history for the given session\n",
    "    return session_store[session]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a8d4e-487f-4b7c-8b65-c14a4a997c2a",
   "metadata": {},
   "source": [
    "## Conversational Chain Setup üîóüí¨\n",
    "\n",
    "In this section, we set up the full conversational chain that connects the retrieval mechanism and the language model for generating answers.\n",
    "\n",
    "### Key Components:\n",
    "- **Retrieval Chain**: Combines the hybrid retriever with the question-answering chain to create a comprehensive retrieval-based question-answering system.\n",
    "- **Session-Aware Chain**: The `RunnableWithMessageHistory` integrates session history, allowing the model to keep track of previous interactions.\n",
    "\n",
    "This creates a seamless conversation flow, where the system can answer based on both previous chat history and relevant retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc522a-08c1-4dd3-a978-63918d6ae79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the stuff documents chain for QA\n",
    "question_answer_chain = create_stuff_documents_chain(llm(), qa_prompt())\n",
    "   \n",
    "# Create the retrieval chain with the history-aware retriever\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever(), question_answer_chain)\n",
    "    \n",
    "# Create a Runnable with message history to allow conversation state management\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",      # Key for the input messages\n",
    "        history_messages_key=\"chat_history\",  # Key for the historical messages\n",
    "        output_messages_key=\"answer\"      # Key for the response\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20229423-b5e2-42fb-9f29-bb8f550e0966",
   "metadata": {},
   "source": [
    "## Main Loop Interaction üßë‚Äçüíª\n",
    "\n",
    "The main loop allows continuous interaction with the assistant. It listens for user input, processes the input using the conversational chain, and returns an answer based on the retrieved context.\n",
    "\n",
    "### Key Functionality:\n",
    "1. **User Input**: The loop takes user input, which is passed to the conversational chain.\n",
    "2. **Response Generation**: The response is generated based on the current session's context, including both the user input and retrieved documents.\n",
    "3. **Exit Condition**: If the user types 'exit', the loop terminates.\n",
    "\n",
    "The assistant responds to the user's queries, using both previous conversations and retrieved documents to provide a relevant and context-aware answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "595450e0-df78-4ee4-87cf-44468ff552b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected PDF:  attention.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question (or type 'exit' to quit):  Introduction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context provided, the dimensions of the input and output in the model are dmodel = 512 and dff = 2048, respectively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question (or type 'exit' to quit):  Explain about Potential Limitations and Future Research Directions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, there is no information regarding potential limitations and future research directions in the given paper. Therefore, I cannot provide any insights on this topic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question (or type 'exit' to quit):  what is Multi-Head Attention?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention is a technique used in the model to allow the attention mechanism to focus on different aspects of the input sequence simultaneously. It allows the model to jointly attend to information from different representation subspaces at different positions. However, this concept is not explained in detail in the provided paper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the conversation. Bye!\n"
     ]
    }
   ],
   "source": [
    "# Main loop for continuous interaction\n",
    "\n",
    "print(\"Selected PDF: \", selected_pdf)\n",
    "while True:\n",
    "    user_input = input(\"\\nYour question (or type 'exit' to quit): \")\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the conversation. Bye!\")        \n",
    "        break\n",
    "\n",
    "    # Retrieve the session's message history\n",
    "    session_history = get_session_history(session_id)\n",
    "\n",
    "    # Invoke the conversational RAG chain with the user input\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "    \n",
    "    answer = response['answer']\n",
    "    assistant_response = answer.split(\"Assistant:\")[-1].strip()\n",
    "    final_answer = assistant_response.split(\"System:\")[-1].split(\"Human:\")[-1].strip()\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d68748-adf9-420c-846c-ab14c5cc50d5",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "There are a few limitations that users may encounter when using the assistant. These limitations are as follows:\n",
    "\n",
    "1. **Token Length**: The \"MAX_LENGTH\" parameter in the code represents the maximum token limit supported by the model (NousResearch-llama-2-7b-chat-hf). As a result, the assistant can process only 3-4 queries at a time. This is because the model needs to load all the retrieved content, history, and corpus into memory, which limits the number of queries it can handle effectively. If you need to process more queries or handle larger content, consider using a model with larger parameters. A bigger model supports a higher token length, allowing it to manage more queries at once.\n",
    "\n",
    "2. **Hallucination**: Occasionally, the assistant may respond with \"I don't know\" even when the question is derived from the corpus. This could happen if the model cannot find the relevant information within the given context. To resolve this issue, try restarting the kernel and re-submit your queries. This helps refresh the model's context and can improve the accuracy of its responses.\n",
    "\n",
    "3. **Out of Memory**: If you encounter an \"Out of Memory\" error, it means the token limit of the model has been exceeded, and there are insufficient resources left to generate a response. In this case, try restarting the kernel and re-submit your queries. If this problem persists, using a model with more parameters may allow for better memory handling and support for longer queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d07123",
   "metadata": {},
   "source": [
    "# Disclaimer for Using Large Language Models\n",
    "Please be aware that while Large Language Models are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "For detailed information on each model's capabilities, licensing, and attribution, please refer to the respective model cards:\n",
    "\n",
    "* **NousResearch-llama-2-7b-chat-hf**\n",
    "  * Model Card: https://huggingface.co/NousResearch/Llama-2-7b-chat-hf\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above. To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel‚Äôs provision of these resources does not expand or otherwise alter Intel‚Äôs applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_rag_venv)",
   "language": "python",
   "name": "llm_rag_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
